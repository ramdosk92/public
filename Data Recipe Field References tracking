import json
import glob
import os
import pandas as pd
from collections import defaultdict
import re

def field_tracking(objt, fld):
    #get all the data recipe json files from the folder 
    recipes_list = glob.glob(r"C:\Users\ram\Downloads\test data recipe/*.json")
    
    # Define path to Desktop and output CSV file location
    desktop_folder = os.path.expanduser("~/Desktop")  # Works on Linux/Mac; on Windows, this resolves to the user's Desktop
    output_file = os.path.join(desktop_folder, 'data_nodes.csv')
    first_write = True  # Overwrite the file initially
    
    #process each recipe file one by one
    for recipe in recipes_list:
        with open(recipe) as file:
            json_data = json.load(file)
        # Removing last 5 characters from the file name to get the recipe Name
        recipe_names = recipe.split('\\')[-1][:-5] 
        
        # Identify load nodes with matching object and field
        sources_node = [node for node in json_data['nodes'] if ((json_data['nodes'][node]['action'])=='load'and (json_data['nodes'][node]['parameters']['dataset'].get('type') == 'connectedDataset')) and (objt is not None and (json_data['nodes'][node]['parameters']['dataset'].get('sourceObjectName')) == objt and fld in (json_data['nodes'][node]['parameters'].get('fields')))]

        # Extract different node types based on action
        filter_node = [node for node in json_data['nodes'] if ((json_data['nodes'][node]['action']) == 'filter')]
        custom_filter_node = [node for node in json_data['nodes'] if ((json_data['nodes'][node]['action']) == 'sqlFilter')]
        transform_nodes = [node for node in json_data['nodes'] if ((json_data['nodes'][node]['action'])=='formula')]
        aggregate_nodes = [node for node in json_data['nodes'] if ((json_data['nodes'][node]['action'])=='aggregate')]
        edit_attributes_nodes = [node for node in json_data['nodes'] if ((json_data['nodes'][node]['action'])=='schema')]
        other_types_nodes = [node for node in json_data['nodes'] if ((json_data['nodes'][node]['action']) in ['formula','filter','schema','aggregate','sqlFilter'])]
        input_nodes = [node for node in json_data['nodes'] if ((json_data['nodes'][node]['action'])=='load')] 

         # Map each join node to its source nodes
        join_nodes = dict() 
        for nd in json_data['nodes']:
            if json_data['nodes'][nd]['action']=='join':
                join_nodes[nd]=json_data['nodes'][nd]['sources']
                

        # Map each TRANSFORM UI node to its formula graph
        formula_node_source_transform_node = {} 
        for node in json_data['ui']['nodes']:
            if json_data['ui']['nodes'][node]['type']== 'TRANSFORM':
                key = node
                for k,v in json_data['ui']['nodes'][node]['graph'].items():
                    formula_node_source_transform_node.setdefault(key,[]).append(k)

        # Map each AGGREGATE UI node to its source node
        aggregate_node_source_node = {}
        for node in json_data['ui']['nodes']:
            if json_data['ui']['nodes'][node]['type']== 'AGGREGATE':
                key = node
                extract_nodes = json_data['ui']['nodes'][node]['graph']
                source_nodes  = list(extract_nodes.keys())[1]
                aggregate_node_source_node.setdefault(node,[]).append(source_nodes)


        # Map all transformation nodes to their input source node
        other_node_source_node = {}   
        for node in other_types_nodes:
            key = node
            other_node_source_node.setdefault(key,[]).append(json_data['nodes'][node]['sources'][0])

        #Initialize lineage tracking structures
        left_join  = set() 
        right_join = set() 
        source_dep = {}    
        initial_sources =[]    
        fields = ["recipe_name","nodes_name","node_label","sub_node_name","field_used","field_name","field_api_name"] # final output file columns name
        data_nodes = {field: [] for field in fields} 
        
        #Assigning input field name into the source dep
        source_dep['actual_value'] = fld # 
        

        # Step 1: Trace downstream from the source node through join and transform nodes (left stream)
        queue = set(sources_node)  

        while queue:
             current = queue.pop()
             found_in_joins = False
             for key, values in join_nodes.items():
                  if values[0] == current and key not in left_join:
                      left_join.add(key)
                      queue.add(key)  # Keep checking this key for more matches in first position
                      found_in_joins = True
                  if values[1] == current:
                      right_join.add(key)
                      source_dep[key] = json_data['nodes'][key]['parameters']['rightQualifier'] + '.' + fld
                      initial_sources.append(key)

            
             if not found_in_joins:
                  for other_key, other_values in other_node_source_node.items():
                      if current in other_values:
                          found_in_formula_node_source_transform_node = False
                          for node_key, node_values in formula_node_source_transform_node.items():
                              if other_key in node_values:
                                  last_value = node_values[-1]
                                  if last_value not in queue and last_value not in left_join:
                                      left_join.add(last_value)
                                      queue.add(last_value)
                                      found_in_formula_node_source_transform_node = True
                          if not found_in_formula_node_source_transform_node:
                              if other_key not in queue and other_key not in left_join:
                                  left_join.add(other_key)
                                  queue.add(other_key)

        # Step 2 & Step 3: Trace bi-directionally across join nodes between left and right streams
        while True:
            prev_left_join = left_join.copy()
            prev_right_join = right_join.copy()
            
            # Step 2: Update left join based on right join 

            queue_left = list(right_join)
            while queue_left:
                current = queue_left.pop()
                found_in_joins = False
                for key, values in join_nodes.items():
                    if values[0] == current and key not in left_join:
                        left_join.add(key)
                        queue_left.append(key)
                        found_in_joins = True
                    if values[1] == current and key not in right_join:
                        right_join.add(key)
                if not found_in_joins:
                    for other_key, other_values in other_node_source_node.items():
                        if current in other_values:
                            found_in_formula_node_source_transform_node = False
                    # extracting transformation nodes final formulas for the left data stream nodes processing
                            for node_key, node_values in formula_node_source_transform_node.items():
                                if other_key in node_values:
                                    last_value = node_values[-1]
                                    if last_value not in queue_left and last_value not in left_join:
                                        left_join.add(last_value)
                                        queue_left.append(last_value)
                                        found_in_formula_node_source_transform_node = True
                            if not found_in_formula_node_source_transform_node:
                                if other_key not in queue_left and other_key not in left_join:
                                    left_join.add(other_key)
                                    queue_left.append(other_key)

                # Step 3 Update right join based on left join
                queue_right = list(left_join)
                while queue_right:
                    current = queue_right.pop()
                    for key, values in join_nodes.items():
                        if values[1] == current and key not in right_join:
                            right_join.add(key)
                            queue_right.append(key)
                         
            # If no new matches are found, stop
            if left_join == prev_left_join and right_join == prev_right_join:
                break

        # Collect right join nodes that are not initial sources
        excep_initial_right_joins = [ node for node in right_join if node not in initial_sources]

        # Map right join nodes to their corresponding input source node
        right_join_source_node = {} 
        formula_to_transform = {}
        for transform, formulas in formula_node_source_transform_node.items():
            for formula in formulas:
                formula_to_transform[formula] = transform
        for node in excep_initial_right_joins:
             for key, values in join_nodes.items():
                if key == node:
                    right_value = values[1]
                    # Check if right_value is a formula, and map to transform if so
                    if right_value in formula_to_transform:
                        right_join_source_node[node] = formula_to_transform[right_value]
                    else:
                        right_join_source_node[node] = right_value
                        



        # step 4 Trace complete data stream lineage from every input node
        data_streams = defaultdict(set)
        for inputs in input_nodes:
            visited = set()
            queue = {inputs}
            while queue:
                current = queue.pop()
                found_in_joins = False
                for key, values in join_nodes.items():
                    if values[0] == current and key not in visited:
                        visited.add(key)
                        queue.add(key)
                        data_streams[inputs].add(key)
                        found_in_joins = True
                if not found_in_joins:
                    for other_key, other_values in other_node_source_node.items():
                        if current in other_values:
                            found_in_formula_node_source_transform_node = False
                            for node_key, node_values in formula_node_source_transform_node.items():
                                if other_key in node_values:
                                    last_value = node_values[-1]
                                    if last_value not in visited:
                                        visited.add(last_value)
                                        queue.add(last_value)
                                        data_streams[inputs].add(node_key)
                                    found_in_formula_node_source_transform_node = True
                                    break
                            # If not found in other_node, add the other_key directly
                            if not found_in_formula_node_source_transform_node:
                                if other_key not in visited:
                                    visited.add(other_key)
                                    queue.add(other_key)
                                    data_streams[inputs].add(other_key)
        
                                    
        # Collect stream for relevant sources(objt)  
        relevant_stream = set()
        for node in sources_node:
            if node in data_streams:
                relevant_stream.update(data_streams[node])
        

        # Step 5: Build dependency mapping for indirect right join nodes
        unresolved = right_join_source_node.copy()
        resolved_keys = set()

        while unresolved:
            next_unresolved = {}
            for k, v in unresolved.items():
                matched_data_stream = [nodes for sources, nodes in data_streams.items() if v in nodes]
                if not matched_data_stream:
                    continue
                nodes_set = matched_data_stream[0]
                rq = json_data['nodes'][k]['parameters'].get('rightQualifier', '')
                if not rq:
                    continue
                relevant_nodes = nodes_set & right_join
                all_concats = []
                still_unresolved = False  # Track if any relevant node in this node is unresolved

                for item in relevant_nodes:
                    if item in source_dep:
                        raw = source_dep[item]
                        vals = raw if isinstance(raw, list) else [raw]
                        all_concats.extend(f"{rq}.{val}" for val in vals)
                    else:
                        next_unresolved[k] = v  
                        still_unresolved = True

                # If at least one relevant node was resolved, update source_dep
                if all_concats:
                    source_dep[k] = all_concats
                    resolved_keys.add(k)
                if not still_unresolved and k not in source_dep:
                    next_unresolved[k] = v
            if unresolved == next_unresolved:
                break
            unresolved = next_unresolved


        # step 6 : Flatten dependency fields
        flat_dep_values = set()
        for val in source_dep.values():
            if isinstance(val,list):
                flat_dep_values.update(val)
            else:
                 flat_dep_values.add(val)
        
        

        
        # step 7 : Processing filter_nodes which has input values
        for node in filter_node:
            # Retrieve the filter expression safely using .get()
            nodes_params = json_data['nodes'][node]['parameters'].get('filterExpressions')
            for flds in nodes_params:
                for values in flat_dep_values:
                    #check match for regular filter nodes
                    if (flds['field']) == values:
                        data_nodes["recipe_name"].append(recipe_names)
                        data_nodes["nodes_name"].append(node)
                        data_nodes["node_label"].append(json_data['ui']['nodes'][node].get('label', ''))
                        data_nodes["sub_node_name"].append(json_data['ui']['nodes'][node].get('label', ''))
                        data_nodes["field_used"].append(values)
                        data_nodes["field_name"].append(values)
                        # Adjust the following if field_api_name is different; otherwise, using field_in_expr
                        data_nodes["field_api_name"].append(values)
                        
            
        # step 8 : Processing customfilter_nodes which has input values
        for node in custom_filter_node:
            fields_list = json_data['nodes'][node]['parameters'].get('sqlFilterExpression', [])
            if fields_list:
                for values in flat_dep_values:
                    pattern = rf"(?<!\w){re.escape(values)}(?!\w)"
                    if re.search(pattern, fields_list):
                        data_nodes["recipe_name"].append(recipe_names)
                        data_nodes["nodes_name"].append(node)
                        data_nodes["node_label"].append(json_data['ui']['nodes'][node].get('label', ''))
                        data_nodes["sub_node_name"].append(json_data['ui']['nodes'][node].get('label', ''))
                        data_nodes["field_used"].append(values)
                        data_nodes["field_name"].append(values)
                        # Adjust the following if field_api_name is different; otherwise, using field_in_expr
                        data_nodes["field_api_name"].append(values)
                

        # step 8 : Processing formula_nodes which has input values
        for node in transform_nodes:
            for values in flat_dep_values:
                # Safely get the fields list (avoid list index error)
                fields_list = json_data['nodes'][node]['parameters'].get('fields', [])
                if fields_list:
                    field_data = fields_list[0]
                    formula_expr = field_data.get('formulaExpression', '')
                    # Safer pattern: match field exactly
                    pattern = rf"(?<!\.)\b{re.escape(values)}\b"

                    if re.search(pattern, formula_expr):
                
                        # Find the label from the mapping; default to empty string if not found
                        labels = next((k for k, v in formula_node_source_transform_node.items() if node in v), '')
                        data_nodes["recipe_name"].append(recipe_names)
                        data_nodes["nodes_name"].append(labels)
                        data_nodes["node_label"].append(json_data['ui']['nodes'][labels].get('label'))
                        data_nodes["sub_node_name"].append(json_data['ui']['nodes'][labels]['graph'][node].get('label'))
                        data_nodes["field_used"].append(values)
                        data_nodes["field_name"].append(field_data.get("label", ""))
                        data_nodes["field_api_name"].append(field_data.get("name", "")) 


        # step 9 : Processing edit_attributes_nodes(schema nodes) -----
        for node in edit_attributes_nodes:
            nodes_params = json_data['nodes'][node]['parameters'] 
            edit_attributes_list = nodes_params.get('fields', [])
            if edit_attributes_list:
                for values in flat_dep_values:
                    #check match for edi attributes nodes
                    for fields in edit_attributes_list:
                        if fields.get('name') == values:
                            # Find the label from the mapping; default to empty string if not found
                            labels = next((k for k, v in formula_node_source_transform_node.items() if node in v), '')
                            data_nodes["recipe_name"].append(recipe_names)
                            data_nodes["nodes_name"].append(labels)
                            data_nodes["node_label"].append(json_data['ui']['nodes'][labels].get('label'))
                            data_nodes["sub_node_name"].append(json_data['ui']['nodes'][labels]['graph'][node].get('label'))
                            data_nodes["field_used"].append(values)
                            data_nodes["field_name"].append(json_data['nodes'][node]['parameters']['fields'][0]['newProperties'].get('label'))
                            data_nodes["field_api_name"].append(json_data['nodes'][node]['parameters']['fields'][0]['newProperties'].get('name')) 


        # step 9 : Processing drop_nodes (schema nodes) -----     
        for node in edit_attributes_nodes:
            nodes_params = json_data['nodes'][node]['parameters']
            drop_node_list = nodes_params.get(('slice'),{}).get('fields', [])
            if drop_node_list :
                for values in flat_dep_values:
                    # check match for Drop nodes
                    for fields in drop_node_list:
                        if fields == values:
                            print(f"field.{fields}")
                            # Find the label from the mapping; default to empty string if not found
                            labels = next((k for k, v in formula_node_source_transform_node.items() if node in v), '')
                            data_nodes["recipe_name"].append(recipe_names)
                            data_nodes["nodes_name"].append(labels)
                            data_nodes["node_label"].append(json_data['ui']['nodes'][labels].get('label'))
                            data_nodes["sub_node_name"].append(json_data['ui']['nodes'][labels]['graph'][node].get('label',''))
                            data_nodes["field_used"].append(values)
                            data_nodes["field_name"].append(json_data['ui']['nodes'][labels]['graph'][node].get('label', ''))
                            data_nodes["field_api_name"].append(json_data['ui']['nodes'][labels]['graph'][node].get('label', ''))
                             
                         
                         
        # step 10 : Processing join_nodes which has input values ----- 
        for node in join_nodes:
            join_params = json_data['nodes'][node]['parameters']
            left_keys = join_params.get('leftKeys',[])
            right_keys = join_params.get('rightKeys',[])
            for values in flat_dep_values:
                if values in left_keys or values in right_keys:
                    data_nodes["recipe_name"].append(recipe_names)
                    data_nodes["nodes_name"].append(node)
                    data_nodes["node_label"].append(json_data['ui']['nodes'][node].get('label', ''))
                    data_nodes["sub_node_name"].append(json_data['ui']['nodes'][node].get('label', ''))
                    data_nodes["field_used"].append(values)
                    data_nodes["field_name"].append(values)
                    data_nodes["field_api_name"].append(values)
                     
                     
        # step 11 : Processing aggregate_nodes which has input values -----
        for node in aggregate_nodes:
            nodes_params = json_data['nodes'][node]['parameters'] 
            group_rows_list = nodes_params.get('groupings', [])
            group_columns_list = nodes_params.get(('pivot_v2'),{}).get('sourceFields', [])
            if group_rows_list or group_columns_list :
                for values in flat_dep_values:
                    #check match for edi attributes nodes
                    for fields in group_rows_list:
                        if fields == values:
                            # Find the label from the mapping; default to empty string if not found
                            label = next((k for k, v in formula_node_source_transform_node.items() if node in v), '')
                            data_nodes["recipe_name"].append(recipe_names)
                            data_nodes["nodes_name"].append(node)
                            data_nodes["node_label"].append(label)
                            data_nodes["sub_node_name"].append(label)
                            data_nodes["field_used"].append(values)
                            data_nodes["field_name"].append(node)
                            data_nodes["field_api_name"].append(node)

                # check match for Drop nodes
                for fields in group_columns_list:
                    if fields == values:
                        # Find the label from the mapping; default to empty string if not found
                        label = next((k for k, v in aggregate_node_source_node.items() if node in v), '')
                        data_nodes["recipe_name"].append(recipe_names)
                        data_nodes["nodes_name"].append(node)
                        data_nodes["node_label"].append(label)
                        data_nodes["sub_node_name"].append(label)
                        data_nodes["field_used"].append(values)
                        data_nodes["field_name"].append(node)
                        data_nodes["field_api_name"].append(node)
                        
        
        df = pd.DataFrame(data_nodes)
        print("source_depsource_dep:{}".format(source_dep))
        
        invalid_field = df["field_used"].isna() | (df["field_used"] == "")
        
        # Filter out invalid node matches for input field
        if relevant_stream:
            condition = ~((df["field_used"]==fld) & (~df["nodes_name"].isin(relevant_stream))) & ~invalid_field
        else:
            condition = df["field_used"] != fld

        #filtered dataframe
        df_filtered = df[condition & ~invalid_field]



        # Write to CSV with overwrite only on the first file,append thereafter
        write_mode = 'w' if first_write else 'a'
        df_filtered.to_csv(output_file, mode=write_mode, header=first_write, index=False)
        first_write = False  # Switch to append mode for subsequent files
        print(df_filtered)
